{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18818,"databundleVersionId":991251,"sourceType":"competition"}],"dockerImageVersionId":29856,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Pneumonia Detection by CNN with Data Augmentation** \n\n2020/03/15\n+ 本範例利用 TensorFlow 2.0 架構下的 `tf.keras` 套件，來建立 CNN 模型，透過胸腔 X-光片影像進行肺炎偵測。\n+ 資料部份：\n    - 將 \"Chest_Xray\" 影像資料匯入，並修改、統一其影像尺寸為 (224, 224, 3)。\n    - 其中，資料匯入區分為 train、test 和 val 三個部分。","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"---------------------------\n## CONTENT\n1. [ APPROACH ](#approach)\n2. [ Data Preprocessing ](#preprocessing)\n    + [ Importing Training Datasets ](#TrainData)\n    + [ Importing Validation Datasets ](#ValData)\n    + [ Importing Test Datasets ](#TestData)\n3. [ CNN Model with *tf.keras* ](#CNNModel)\n    + [ Forward Propagation ](#Forwardpropagation)\n    + [ Model Summary & Plotting the Model ](#ModelSummary)\n- [ Start Training - from Coarse to Fine ](#StartTraining)\n    + [ Setting Hyperparameters ](#SettingHyperparameters)\n    + [ STAGE 1 - Coarse Training without Data Augmentation ](#Stage1)\n        + [ Backpropagation for STAGE 1 ](#Backpropagation1)\n    + [ STAGE 2 - Fine Training *with* Data Augmentation ](#Stage2)\n        + [ Backpropagation for STAGE 2 ](#Backpropagation2)\n- [ Saving the Entire Model with HDF5 Format ](#SavingEntireModel)\n---------------------------","metadata":{}},{"cell_type":"markdown","source":"<a id='approach'></a>\n## 1. APPROACH\n> + 本範例採用 **From Coarse to Fine** 的調校過程 (Tuning Process)：\n    - 首先，利用原始影像資料(raw images)進行 CNN Model 參數調校，並輸出結果 (參考 < STAGE 1 : Coarse Training without Data Augmentation > 部份)。\n    - 之後，利用 \"**資料擴增 (Data Augmentation)**\" 技術，在程式執行階段 (runtime) 增加 Training 資料量，再次對 CNN Model 進行參數調校，並輸出結果 (參考 < STAGE 2 > 部份)。\n> + 其**目的**是在 \"**有效縮短運算時間，減少佔用記憶體空間，同時提升預測結果的準確度**\"，達成整體效能(performance)的提升。\n* > + 本範例中，**CNN Model** 採用 \"Batch Normalization\"、\"Dropout\"以及 \"Learning-Rate Decay\" 等技術。\n> + 下列程式同時輸出 < STAGE 1 > 和 < STAGE 2 > 結果做為參考，並輸出、儲存整個預測模型。","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-11-22T12:05:50.371844Z","iopub.execute_input":"2021-11-22T12:05:50.372499Z","iopub.status.idle":"2021-11-22T12:05:56.577467Z","shell.execute_reply.started":"2021-11-22T12:05:50.372354Z","shell.execute_reply":"2021-11-22T12:05:56.576764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix\n\nimport cv2\nimport os\nimport glob\n\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom PIL import Image\nfrom pathlib import Path\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, SeparableConv2D\nfrom tensorflow.keras.layers import GlobalMaxPooling2D, Flatten, Dropout\nfrom tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:56.579476Z","iopub.execute_input":"2021-11-22T12:05:56.5798Z","iopub.status.idle":"2021-11-22T12:05:57.20597Z","shell.execute_reply.started":"2021-11-22T12:05:56.579752Z","shell.execute_reply":"2021-11-22T12:05:57.205245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='preprocessing'></a>\n## 2. Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\nINPUT_PATH = \"../input/pneumonia-detection/chest_xray\"\n\n# List the files in the input directory.\nprint(os.listdir(INPUT_PATH))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:57.208533Z","iopub.execute_input":"2021-11-22T12:05:57.209082Z","iopub.status.idle":"2021-11-22T12:05:57.227506Z","shell.execute_reply.started":"2021-11-22T12:05:57.20903Z","shell.execute_reply":"2021-11-22T12:05:57.226579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### [ File Directory ]\n       +-- input \n             |-- pneumonia-detection\n                  |-- chest_xray\n                         |-- test\n                               |-- NORMAL\n                               |-- PNEUMONIA\n                         |-- train\n                               |-- NORMAL\n                               |-- PNEUMONIA\n                         |-- val\n                               |-- NORMAL\n                               |-- PNEUMONIA\n","metadata":{}},{"cell_type":"markdown","source":"### Training Datasets","metadata":{}},{"cell_type":"code","source":"# list of all the training images\ntrain_normal = Path(INPUT_PATH + '/train/NORMAL').glob('*.jpeg')\ntrain_pneumonia = Path(INPUT_PATH + '/train/PNEUMONIA').glob('*.jpeg')\n\n# ---------------------------------------------------------------\n# Train data format in (img_path, label) \n# Labels for [ the normal cases = 0 ] & [the pneumonia cases = 1]\n# ---------------------------------------------------------------\nnormal_data = [(image, 0) for image in train_normal]\npneumonia_data = [(image, 1) for image in train_pneumonia]\n\ntrain_data = normal_data + pneumonia_data\n\n# Get a pandas dataframe from the data we have in our list \ntrain_data = pd.DataFrame(train_data, columns=['image', 'label'])\n\n# Checking the dataframe...\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:57.229032Z","iopub.execute_input":"2021-11-22T12:05:57.229843Z","iopub.status.idle":"2021-11-22T12:05:57.470123Z","shell.execute_reply.started":"2021-11-22T12:05:57.229529Z","shell.execute_reply":"2021-11-22T12:05:57.469486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the dataframe...\ntrain_data.tail()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:57.473157Z","iopub.execute_input":"2021-11-22T12:05:57.473429Z","iopub.status.idle":"2021-11-22T12:05:57.481238Z","shell.execute_reply.started":"2021-11-22T12:05:57.473382Z","shell.execute_reply":"2021-11-22T12:05:57.480462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle the data \ntrain_data = train_data.sample(frac=1., random_state=100).reset_index(drop=True)\n\n# Checking the dataframe...\ntrain_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:57.484101Z","iopub.execute_input":"2021-11-22T12:05:57.484513Z","iopub.status.idle":"2021-11-22T12:05:57.501082Z","shell.execute_reply.started":"2021-11-22T12:05:57.484462Z","shell.execute_reply":"2021-11-22T12:05:57.500471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:57.502535Z","iopub.execute_input":"2021-11-22T12:05:57.502971Z","iopub.status.idle":"2021-11-22T12:05:57.511166Z","shell.execute_reply.started":"2021-11-22T12:05:57.502798Z","shell.execute_reply":"2021-11-22T12:05:57.510416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counts for both classes\ncount_result = train_data['label'].value_counts()\nprint('Total of Train Data : ', len(train_data), '  (0 : Normal; 1 : Pneumonia)')\nprint(count_result)\n\n# Plot the results \nplt.figure(figsize=(8,5))\nsns.countplot(x = 'label', data =  train_data)\nplt.title('Number of classes', fontsize=16)\nplt.xlabel('Class type', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(range(len(count_result.index)), \n           ['Normal : 0', 'Pneumonia : 1'], \n           fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:57.512458Z","iopub.execute_input":"2021-11-22T12:05:57.512895Z","iopub.status.idle":"2021-11-22T12:05:57.825137Z","shell.execute_reply.started":"2021-11-22T12:05:57.512849Z","shell.execute_reply":"2021-11-22T12:05:57.824336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 4, figsize=(20,15))\nfor i, axi in enumerate(ax.flat):\n    image = imread(train_data.image[i])\n    axi.imshow(image, cmap='bone')\n    axi.set_title(('Normal' if train_data.label[i] == 0 else 'Pneumonia') \n                  + '  [size=' + str(image.shape) +']',\n                  fontsize=14)\n    axi.set(xticks=[], yticks=[])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:57.826468Z","iopub.execute_input":"2021-11-22T12:05:57.826891Z","iopub.status.idle":"2021-11-22T12:05:59.295852Z","shell.execute_reply.started":"2021-11-22T12:05:57.826843Z","shell.execute_reply":"2021-11-22T12:05:59.294897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.to_numpy().shape","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:59.297077Z","iopub.execute_input":"2021-11-22T12:05:59.297328Z","iopub.status.idle":"2021-11-22T12:05:59.305222Z","shell.execute_reply.started":"2021-11-22T12:05:59.297291Z","shell.execute_reply":"2021-11-22T12:05:59.304448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import X-ray Image Datasets from /train, /val & /test","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------------------------------------\n#  Loading X-ray Images datasets from file 3 directories, respectively. \n# ----------------------------------------------------------------------\ndef load_data(files_dir='/train'):\n    # list of the paths of all the image files\n    normal = Path(INPUT_PATH + files_dir + '/NORMAL').glob('*.jpeg')\n    pneumonia = Path(INPUT_PATH + files_dir + '/PNEUMONIA').glob('*.jpeg')\n\n    # --------------------------------------------------------------\n    # Data-paths' format in (img_path, label) \n    # labels : for [ Normal cases = 0 ] & [ Pneumonia cases = 1 ]\n    # --------------------------------------------------------------\n    normal_data = [(image, 0) for image in normal]\n    pneumonia_data = [(image, 1) for image in pneumonia]\n\n    image_data = normal_data + pneumonia_data\n\n    # Get a pandas dataframe for the data paths \n    image_data = pd.DataFrame(image_data, columns=['image', 'label'])\n    \n    # Shuffle the data \n    image_data = image_data.sample(frac=1., random_state=100).reset_index(drop=True)\n    \n    # Importing both image & label datasets...\n    x_images, y_labels = ([data_input(image_data.iloc[i][:]) for i in range(len(image_data))], \n                         [image_data.iloc[i][1] for i in range(len(image_data))])\n\n    # Convert the list into numpy arrays\n    x_images = np.array(x_images)\n    y_labels = np.array(y_labels)\n    \n    print(\"Total number of images: \", x_images.shape)\n    print(\"Total number of labels: \", y_labels.shape)\n    \n    return x_images, y_labels","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:59.307329Z","iopub.execute_input":"2021-11-22T12:05:59.307949Z","iopub.status.idle":"2021-11-22T12:05:59.32114Z","shell.execute_reply.started":"2021-11-22T12:05:59.307895Z","shell.execute_reply":"2021-11-22T12:05:59.32042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------------------------------------\n#  1. Resizing all the images to 224x224 with 3 channels.\n#  2. Then, normalize the pixel values.  \n# ---------------------------------------------------------\ndef data_input(dataset):\n    # print(dataset.shape)\n    for image_file in dataset:\n        image = cv2.imread(str(image_file))\n        image = cv2.resize(image, (224,224))\n        if image.shape[2] == 1:\n            # np.dstack(): Stack arrays in sequence depth-wise \n            #              (along third axis).\n            # https://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html\n            image = np.dstack([image, image, image])\n        \n        # ----------------------------------------------------------\n        # cv2.cvtColor(): The function converts an input image \n        #                 from one color space to another. \n        # [Ref.1]: \"cvtColor - OpenCV Documentation\"\n        #     - https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html\n        # [Ref.2]: \"Python计算机视觉编程- 第十章 OpenCV\" \n        #     - https://yongyuan.name/pcvwithpython/chapter10.html\n        # ----------------------------------------------------------\n        x_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Normalization\n        x_image = x_image.astype(np.float32)/255.\n        return x_image","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:59.322637Z","iopub.execute_input":"2021-11-22T12:05:59.323321Z","iopub.status.idle":"2021-11-22T12:05:59.33242Z","shell.execute_reply.started":"2021-11-22T12:05:59.323113Z","shell.execute_reply":"2021-11-22T12:05:59.331431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TrainData\"></a>\n+ ### Importing Training Datasets","metadata":{}},{"cell_type":"code","source":"# Import train dataset...\nx_train, y_train = load_data(files_dir='/train')\n\nprint(x_train.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:59.334187Z","iopub.execute_input":"2021-11-22T12:05:59.334921Z","iopub.status.idle":"2021-11-22T12:07:35.412704Z","shell.execute_reply.started":"2021-11-22T12:05:59.334866Z","shell.execute_reply":"2021-11-22T12:07:35.412018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:35.4145Z","iopub.execute_input":"2021-11-22T12:07:35.41496Z","iopub.status.idle":"2021-11-22T12:07:35.423022Z","shell.execute_reply.started":"2021-11-22T12:07:35.41491Z","shell.execute_reply":"2021-11-22T12:07:35.422273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:35.424808Z","iopub.execute_input":"2021-11-22T12:07:35.425419Z","iopub.status.idle":"2021-11-22T12:07:35.435632Z","shell.execute_reply.started":"2021-11-22T12:07:35.425299Z","shell.execute_reply":"2021-11-22T12:07:35.434797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:35.437583Z","iopub.execute_input":"2021-11-22T12:07:35.438033Z","iopub.status.idle":"2021-11-22T12:07:35.445296Z","shell.execute_reply.started":"2021-11-22T12:07:35.437849Z","shell.execute_reply":"2021-11-22T12:07:35.444148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ValData\"></a>\n+ ### Importing Validation Datasets","metadata":{}},{"cell_type":"code","source":"# Import validation dataset...\nx_val, y_val = load_data(files_dir='/val')\n\nprint(x_val.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:35.447278Z","iopub.execute_input":"2021-11-22T12:07:35.447995Z","iopub.status.idle":"2021-11-22T12:07:35.739011Z","shell.execute_reply.started":"2021-11-22T12:07:35.447942Z","shell.execute_reply":"2021-11-22T12:07:35.737501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:35.740396Z","iopub.execute_input":"2021-11-22T12:07:35.74068Z","iopub.status.idle":"2021-11-22T12:07:35.749012Z","shell.execute_reply.started":"2021-11-22T12:07:35.740634Z","shell.execute_reply":"2021-11-22T12:07:35.748018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TestData\"></a>\n+ ### Importing Test Datasets","metadata":{}},{"cell_type":"code","source":"# Import test dataset...\nx_test, y_test = load_data(files_dir='/test')\n\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:35.750609Z","iopub.execute_input":"2021-11-22T12:07:35.751066Z","iopub.status.idle":"2021-11-22T12:07:45.448623Z","shell.execute_reply.started":"2021-11-22T12:07:35.750888Z","shell.execute_reply":"2021-11-22T12:07:45.44696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counts for both classes\ncount_result = pd.Series(y_test).value_counts()\nprint('Total of Test Data : ', len(y_test), '  (0 : Normal; 1 : Pneumonia)')\nprint('------------------')\nprint(count_result)\nprint('------------------')\nprint('1 :  ', count_result[1]/sum(count_result))\nprint('0 :  ', count_result[0]/sum(count_result))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:45.450039Z","iopub.execute_input":"2021-11-22T12:07:45.45036Z","iopub.status.idle":"2021-11-22T12:07:45.462195Z","shell.execute_reply.started":"2021-11-22T12:07:45.450303Z","shell.execute_reply":"2021-11-22T12:07:45.4612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:45.463337Z","iopub.execute_input":"2021-11-22T12:07:45.463675Z","iopub.status.idle":"2021-11-22T12:07:45.476324Z","shell.execute_reply.started":"2021-11-22T12:07:45.463629Z","shell.execute_reply":"2021-11-22T12:07:45.475648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='CNNModel'></a>\n## 3. CNN Model by *tf.keras*","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Forwardpropagation\"></a>\n+ ### Forward Propagation - with Batch Normalization and Dropout\n    + Conv2D layer\n    + 1x1 Convolution ([Ref]: Prof Andrew Ng, \"Inception Module\", https://www.youtube.com/watch?v=KfV8CJh7hE0)","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    Conv2D(32, (5,5), activation='relu', padding='same', \n           input_shape=(224,224,3), name='Conv1_1'),\n    BatchNormalization(name='bn1_1'),\n    Conv2D(32, (5,5), activation='relu', padding='same', name='Conv1_2'),\n    BatchNormalization(name='bn1_2'),\n    Conv2D(32, (5,5), activation='relu', padding='same', name='Conv1_3'),\n    BatchNormalization(name='bn1_3'),\n    MaxPooling2D((2,2), name='MaxPool1'),\n    Dropout(0.25),\n    \n    Conv2D(48, (3,3), activation='relu', padding='same', name='Conv2_1'),\n    BatchNormalization(name='bn2_1'),\n    Conv2D(48, (3,3), activation='relu', padding='same', name='Conv2_2'),\n    BatchNormalization(name='bn2_2'),\n    Conv2D(48, (3,3), activation='relu', padding='same', name='Conv2_3'),\n    BatchNormalization(name='bn2_3'),    \n    MaxPooling2D((2,2), name='MaxPool2'),\n    Dropout(0.25),\n\n    Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3_1'),\n    BatchNormalization(name='bn3_1'),\n    Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3_2'),\n    BatchNormalization(name='bn3_2'),\n    Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3_3'),\n    BatchNormalization(name='bn3_3'),\n    MaxPooling2D((2,2), name='MaxPool3'),\n    Dropout(0.25),\n    \n    # ----------------------------------------------------------------------\n    # Using \"1x1 convolution layer\" to lower the complexity of computing\n    # [Ref]: Prof Andrew Ng, \"Inception Module\", \n    #        https://www.youtube.com/watch?v=KfV8CJh7hE0\n    # ----------------------------------------------------------------------\n    Conv2D(16, (1,1), activation='relu', padding='same', name='Conv4_1_1x1'),\n    BatchNormalization(name='bn4_1_1x1'),\n    Conv2D(64, (3,3), activation='relu', padding='same', name='Conv4_2'),\n    BatchNormalization(name='bn4_2'),\n    Conv2D(32, (1,1), activation='relu', padding='same', name='Conv4_3_1x1'),\n    BatchNormalization(name='bn4_3_1x1'),\n    Conv2D(128, (3,3), activation='relu', padding='same', name='Conv4_4'),\n    BatchNormalization(name='bn4_4'),\n    MaxPooling2D((2,2), name='MaxPool4'),\n    Dropout(0.25),\n\n    # Using \"1x1 convolution layer\" \n    Conv2D(32, (1,1), activation='relu', padding='same', name='Conv5_1_1x1'),\n    BatchNormalization(name='bn5_1_1x1'),\n    Conv2D(128, (3,3), activation='relu', padding='same', name='Conv5_2'),\n    BatchNormalization(name='bn5_2'),\n    Conv2D(64, (1,1), activation='relu', padding='same', name='Conv5_3_1x1'),\n    BatchNormalization(name='bn5_3_1x1'),\n    Conv2D(256, (3,3), activation='relu', padding='same', name='Conv5_4'),\n    BatchNormalization(name='bn5_4'),\n    MaxPooling2D((2,2), name='MaxPool5'),\n    Dropout(0.25),\n    \n    # Using \"1x1 convolution layer\" \n    Conv2D(64, (1,1), activation='relu', padding='same', name='Conv6_1x1'),\n    BatchNormalization(name='bn6_1x1'),\n    Conv2D(512, (3,3), activation='relu', padding='same', name='Conv6_2'),\n    BatchNormalization(name='bn6_2'),\n    \n    Conv2D(128, (1,1), activation='relu', padding='same', name='Conv7_1x1'),\n    BatchNormalization(name='bn7_1x1'),\n    Conv2D(1024, (3,3), activation='relu', name='Conv7_2'),\n    BatchNormalization(name='bn7_3'),\n    GlobalAveragePooling2D(name='GlobalAveragePool_1'),\n    Dropout(0.5),\n    \n    #Flatten(),\n    #Dense(64, activation='relu', name='fc'), \n    #BatchNormalization(name='bn_fc'),\n    #Dropout(0.5),\n    Dense(1, name='Output')   #  activation='sigmoid' =>  BinaryCrossentropy(logits=True)\n])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:45.479736Z","iopub.execute_input":"2021-11-22T12:07:45.479963Z","iopub.status.idle":"2021-11-22T12:07:48.510377Z","shell.execute_reply.started":"2021-11-22T12:07:45.479914Z","shell.execute_reply":"2021-11-22T12:07:48.509527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ModelSummary\"></a>\n+ ### Model Summary & Plotting the Model","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:48.511832Z","iopub.execute_input":"2021-11-22T12:07:48.512104Z","iopub.status.idle":"2021-11-22T12:07:48.53489Z","shell.execute_reply.started":"2021-11-22T12:07:48.512062Z","shell.execute_reply":"2021-11-22T12:07:48.534152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> + ### 此處可見 CNN Model 有 2,669,361 個參數需要進行訓練、調校！\n","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, dpi=85)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:48.538024Z","iopub.execute_input":"2021-11-22T12:07:48.538246Z","iopub.status.idle":"2021-11-22T12:07:49.892959Z","shell.execute_reply.started":"2021-11-22T12:07:48.538205Z","shell.execute_reply":"2021-11-22T12:07:49.892047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"StartTraining\"></a>\n## 4. Start Training with Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"<a id=\"SettingHyperparameters\"></a>\n+ ### Setting Hyperparameters for Training Process\n>+ 由於 validation dataset 只有 16 筆影像資料 (太少了些)，因此，直接將 training datasets (5216 images) 分割出 4200 筆的 training 資料 (80.5% 資料量)，其餘的 1016 張 X-ray 影像資料做為 validation 資料集。\n>+ **[ Learning-Rate Decay ] : basic learning rate = 0.001**\n    + 在 STAGE 1 中，設定 basic learning rate 的 1/10 為 decay rate，取 epoch 數為 10 時，每個 epoch 的 GPU 運算時間約為 20 秒。\n    + 在 STAGE 2 中，設定 learning_rate 為 basic learning rate 的 1/10 (decay rate =  learning_rate/100)，取 epoch 數為 20，每個 epoch 的 GPU 運算時間約為 45 秒。","metadata":{}},{"cell_type":"code","source":"batch_size = 16\nepochs_stage_1 = 10\nepochs_stage_2 = 20\ntrain_data_num = 4200","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:49.894576Z","iopub.execute_input":"2021-11-22T12:07:49.895099Z","iopub.status.idle":"2021-11-22T12:07:49.899733Z","shell.execute_reply.started":"2021-11-22T12:07:49.895046Z","shell.execute_reply":"2021-11-22T12:07:49.89867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Stage1\"></a>\n+ ### STAGE 1 : Coarse Training *without* Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Backpropagation1\"></a>\n> + ### Backpropagation - *Optimizer*, *Loss Function* & *Accuracy* for < STAGE 1 >","metadata":{}},{"cell_type":"code","source":"# Adam Optimizer with Learning-rate Decay \nbasic_learning_rate = 0.001\nopt = Adam(lr=basic_learning_rate, decay=basic_learning_rate/10.)\n\nmodel.compile(optimizer=opt,\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:49.901094Z","iopub.execute_input":"2021-11-22T12:07:49.901631Z","iopub.status.idle":"2021-11-22T12:07:49.96295Z","shell.execute_reply.started":"2021-11-22T12:07:49.901583Z","shell.execute_reply":"2021-11-22T12:07:49.962072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## data_augmentation = False\nprint('Not using data augmentation.')\nepochs = epochs_stage_1\nhistory_no_data_aug = model.fit(x_train[:train_data_num], y_train[:train_data_num],\n                               batch_size=batch_size,\n                               epochs=epochs,\n                               validation_data=(x_train[train_data_num:], y_train[train_data_num:]),\n                               # validation_data=(x_val, y_val),\n                               shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:49.965661Z","iopub.execute_input":"2021-11-22T12:07:49.966337Z","iopub.status.idle":"2021-11-22T12:11:45.242404Z","shell.execute_reply.started":"2021-11-22T12:07:49.966189Z","shell.execute_reply":"2021-11-22T12:11:45.241687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_no_data_aug.history.keys()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:45.244071Z","iopub.execute_input":"2021-11-22T12:11:45.244385Z","iopub.status.idle":"2021-11-22T12:11:45.251395Z","shell.execute_reply.started":"2021-11-22T12:11:45.244334Z","shell.execute_reply":"2021-11-22T12:11:45.250612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Validation-Curve Diagrams for STAGE 1 - Training without Data Augmentation","metadata":{}},{"cell_type":"code","source":"acc = history_no_data_aug.history['accuracy']\nval_acc = history_no_data_aug.history['val_accuracy']\n\nloss = history_no_data_aug.history['loss']\nval_loss = history_no_data_aug.history['val_loss']\n\nepochs_range = range(1, epochs + 1)\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylim(0, 1)\nplt.xticks(epochs_range)\nplt.title('Training and Validation Accuracy - without Data Augmentation')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylim(0, 1)\nplt.xticks(epochs_range)\nplt.title('Training and Validation Loss - without Data Augmentation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:45.252858Z","iopub.execute_input":"2021-11-22T12:11:45.253403Z","iopub.status.idle":"2021-11-22T12:11:45.707587Z","shell.execute_reply.started":"2021-11-22T12:11:45.253353Z","shell.execute_reply":"2021-11-22T12:11:45.706654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Evaluation for Test Datasets in < STAGE 1 > ","metadata":{}},{"cell_type":"code","source":"# Score trained model.\nloss, acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', loss)\nprint('Test accuracy:', acc)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:45.708865Z","iopub.execute_input":"2021-11-22T12:11:45.709177Z","iopub.status.idle":"2021-11-22T12:11:47.014405Z","shell.execute_reply.started":"2021-11-22T12:11:45.709131Z","shell.execute_reply":"2021-11-22T12:11:47.013388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions\npreds = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:47.01593Z","iopub.execute_input":"2021-11-22T12:11:47.016398Z","iopub.status.idle":"2021-11-22T12:11:48.494036Z","shell.execute_reply.started":"2021-11-22T12:11:47.016285Z","shell.execute_reply":"2021-11-22T12:11:48.492891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:48.502098Z","iopub.execute_input":"2021-11-22T12:11:48.502416Z","iopub.status.idle":"2021-11-22T12:11:48.514315Z","shell.execute_reply.started":"2021-11-22T12:11:48.50237Z","shell.execute_reply":"2021-11-22T12:11:48.509857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\nfor i in range(len(preds)):\n    if preds[i] > 0.5 : \n        y_pred.append(1) \n    else: \n        y_pred.append(0)\n        \nprint(' y_pred = ', np.array(y_pred[:10]))\nprint(' y_test = ', y_test[:10])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:48.51584Z","iopub.execute_input":"2021-11-22T12:11:48.516273Z","iopub.status.idle":"2021-11-22T12:11:48.539234Z","shell.execute_reply.started":"2021-11-22T12:11:48.516224Z","shell.execute_reply":"2021-11-22T12:11:48.535493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Confusion Matrix for < STAGE 1 >\n>                        ---------------------------------\n>                        |               |               |\n>                        |     true      |     false     |\n>           Normal : 0   |   negative    |    positive   |\n>                        |     (tn)      |      (fp)     |\n>      true              |               |               |\n>      value             ---------------------------------\n>                        |               |               |\n>                        |    false      |     true      |\n>         Pneumonia : 1  |   negative    |    positive   |\n>                        |     (fn)      |      (tp)     |\n>                        |               |               |\n>                        ---------------------------------\n>                            Normal : 0     Pneumonia : 1\n>\n>                                 predicted value","metadata":{}},{"cell_type":"code","source":"mat = confusion_matrix(y_test, y_pred)\nprint(mat)\n\nplt.figure(figsize=(8,6))\nsns.heatmap(mat, square=False, annot=True, fmt ='d', cbar=True, annot_kws={\"size\": 16})\nplt.title('0 : Normal   1 : Pneumonia', fontsize = 20)\nplt.xticks(fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.xlabel('predicted value', fontsize = 20)\nplt.ylabel('true value', fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:48.541277Z","iopub.execute_input":"2021-11-22T12:11:48.542038Z","iopub.status.idle":"2021-11-22T12:11:49.038847Z","shell.execute_reply.started":"2021-11-22T12:11:48.541991Z","shell.execute_reply":"2021-11-22T12:11:49.037989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Calculating *precision*, *recall*, *accuracy*, *F1_score* & *F2_score* for < STAGE 1 >","metadata":{}},{"cell_type":"code","source":"# Calculate Precision and Recall\ntn, fp, fn, tp = mat.ravel()\nprint('tn = {}, fp = {}, fn = {}, tp = {} '.format(tn, fp, fn, tp))\n\nprecision = tp/(tp+fp)\nrecall = tp/(tp+fn)\naccuracy = (tp+tn)/(tp+tn+fp+fn)\nf1_score = 2. * precision * recall / (precision + recall)\nf2_score = 5. * precision * recall / (4. * precision + recall)\n\nprint(\"\\nTest Recall of the model \\t = {:.4f}\".format(recall))\nprint(\"Test Precision of the model \\t = {:.4f}\".format(precision))\nprint(\"Test Accuracy of the model \\t = {:.4f}\".format(accuracy))\nprint(\"\\nTest F1 score of the model \\t = {:.4f}\".format(f1_score))\nprint(\"\\nTest F2 score of the model \\t = {:.4f}\".format(f2_score))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:49.043512Z","iopub.execute_input":"2021-11-22T12:11:49.045918Z","iopub.status.idle":"2021-11-22T12:11:49.063501Z","shell.execute_reply.started":"2021-11-22T12:11:49.045855Z","shell.execute_reply":"2021-11-22T12:11:49.062686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Stage2\"></a>\n+ ### STAGE 2 : Fine Training *with* Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Backpropagation2\"></a>\n> + ### Backpropagation - *Optimizer*, *Loss Function* & *Accuracy* for < STAGE 2 >","metadata":{}},{"cell_type":"code","source":"# Adam Optimizer with Learning-rate Decay \nlr_with_decay = basic_learning_rate / 10.\nopt = Adam(lr=lr_with_decay, decay=lr_with_decay/100.)\n\nmodel.compile(optimizer=opt,\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:49.068244Z","iopub.execute_input":"2021-11-22T12:11:49.070891Z","iopub.status.idle":"2021-11-22T12:11:49.21897Z","shell.execute_reply.started":"2021-11-22T12:11:49.070807Z","shell.execute_reply":"2021-11-22T12:11:49.218211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Data Augmentation Function","metadata":{}},{"cell_type":"code","source":"def data_augm():\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        # randomly shift images horizontally (fraction of total width)\n        width_shift_range=0.05,\n        # randomly shift images vertically (fraction of total height)\n        height_shift_range=0.05,\n        # rotation_range=20,\n        horizontal_flip=True,  # Randomly flip inputs horizontally.\n        # vertical_flip=True,  # Randomly flip inputs vertically.\n        # zoom_range=[0.95, 1.05] # Range for random zoom\n    )\n    return datagen","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:49.220297Z","iopub.execute_input":"2021-11-22T12:11:49.220594Z","iopub.status.idle":"2021-11-22T12:11:49.225919Z","shell.execute_reply.started":"2021-11-22T12:11:49.220522Z","shell.execute_reply":"2021-11-22T12:11:49.224979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Training with Data Augmentation","metadata":{}},{"cell_type":"code","source":"print('With data augmentation.')\ndatagen = data_augm()\nepochs = epochs_stage_2\n\n# Compute quantities required for feature-wise normalization\n# (std, mean, and principal components if ZCA whitening is applied).\ndatagen.fit(x_train[:train_data_num])\n\n# Fit the model on the batches generated by datagen.flow().\nhistory_data_aug = model.fit_generator(datagen.flow(x_train[:train_data_num], y_train[:train_data_num], \n                                                    batch_size=batch_size),\n                                                    epochs=epochs,\n                                                    validation_data=(x_train[train_data_num:], y_train[train_data_num:]),\n                                                    # validation_data=(x_val, y_val),\n                                                    workers=4)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:11:49.227456Z","iopub.execute_input":"2021-11-22T12:11:49.228059Z","iopub.status.idle":"2021-11-22T12:26:25.179706Z","shell.execute_reply.started":"2021-11-22T12:11:49.22801Z","shell.execute_reply":"2021-11-22T12:26:25.178164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Validation-Curve Diagrams for STAGE 2 - Training *with* Data Augmentation","metadata":{}},{"cell_type":"code","source":"acc = history_data_aug.history['accuracy']\nval_acc = history_data_aug.history['val_accuracy']\n\nloss = history_data_aug.history['loss']\nval_loss = history_data_aug.history['val_loss']\n\nepochs_range = range(1, epochs + 1)\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylim(0, 1)\nplt.xticks(epochs_range)\nplt.title('Training and Validation Accuracy with Data Augmentation')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylim(0, 1)\nplt.xticks(epochs_range)\nplt.title('Training and Validation Loss with Data Augmentation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:25.184052Z","iopub.execute_input":"2021-11-22T12:26:25.186142Z","iopub.status.idle":"2021-11-22T12:26:26.338437Z","shell.execute_reply.started":"2021-11-22T12:26:25.184288Z","shell.execute_reply":"2021-11-22T12:26:26.337606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Evaluation with Test Dataset for < STAGE 2 >","metadata":{}},{"cell_type":"code","source":"# Score trained model.\nloss, acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', loss)\nprint('Test accuracy:', acc)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:26.339803Z","iopub.execute_input":"2021-11-22T12:26:26.340073Z","iopub.status.idle":"2021-11-22T12:26:27.424083Z","shell.execute_reply.started":"2021-11-22T12:26:26.340028Z","shell.execute_reply":"2021-11-22T12:26:27.42338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions\npreds = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:27.426486Z","iopub.execute_input":"2021-11-22T12:26:27.426955Z","iopub.status.idle":"2021-11-22T12:26:28.817731Z","shell.execute_reply.started":"2021-11-22T12:26:27.4269Z","shell.execute_reply":"2021-11-22T12:26:28.816939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:28.819083Z","iopub.execute_input":"2021-11-22T12:26:28.819362Z","iopub.status.idle":"2021-11-22T12:26:28.828514Z","shell.execute_reply.started":"2021-11-22T12:26:28.819318Z","shell.execute_reply":"2021-11-22T12:26:28.827572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\nfor i in range(len(preds)):\n    if preds[i] > 0.5 : \n        y_pred.append(1) \n    else: \n        y_pred.append(0)\n        \nprint(' y_pred = ', np.array(y_pred[:10]))\nprint(' y_test = ', y_test[:10])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:28.830122Z","iopub.execute_input":"2021-11-22T12:26:28.830663Z","iopub.status.idle":"2021-11-22T12:26:28.840505Z","shell.execute_reply.started":"2021-11-22T12:26:28.830614Z","shell.execute_reply":"2021-11-22T12:26:28.839476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Confusion Matrix for < STAGE 2 >\n>                        ---------------------------------\n>                        |               |               |\n>                        |     true      |     false     |\n>           Normal : 0   |   negative    |    positive   |\n>                        |     (tn)      |      (fp)     |\n>      true              |               |               |\n>      value             ---------------------------------\n>                        |               |               |\n>                        |    false      |     true      |\n>         Pneumonia : 1  |   negative    |    positive   |\n>                        |     (fn)      |      (tp)     |\n>                        |               |               |\n>                        ---------------------------------\n>                            Normal : 0     Pneumonia : 1\n>\n>                                 predicted value","metadata":{}},{"cell_type":"code","source":"mat = confusion_matrix(y_test, y_pred)\nprint(mat)\n\nplt.figure(figsize=(8,6))\nsns.heatmap(mat, square=False, annot=True, fmt ='d', cbar=True, annot_kws={\"size\": 16})\nplt.title('0 : Normal   1 : Pneumonia', fontsize = 20)\nplt.xticks(fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.xlabel('predicted value', fontsize = 20)\nplt.ylabel('true value', fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:28.842256Z","iopub.execute_input":"2021-11-22T12:26:28.842927Z","iopub.status.idle":"2021-11-22T12:26:29.125899Z","shell.execute_reply.started":"2021-11-22T12:26:28.842517Z","shell.execute_reply":"2021-11-22T12:26:29.124932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculating *precision*, *recall*, *accuracy*, *F1_score* & *F2_score* for < STAGE 2 >","metadata":{}},{"cell_type":"code","source":"# Calculate Precision and Recall\ntn, fp, fn, tp = mat.ravel()\nprint('tn = {}, fp = {}, fn = {}, tp = {} '.format(tn, fp, fn, tp))\n\nprecision = tp/(tp+fp)\nrecall = tp/(tp+fn)\naccuracy = (tp+tn)/(tp+tn+fp+fn)\nf1_score = 2. * precision * recall / (precision + recall)\nf2_score = 5. * precision * recall / (4. * precision + recall)\n\nprint(\"\\nTest Recall of the model \\t = {:.4f}\".format(recall))\nprint(\"Test Precision of the model \\t = {:.4f}\".format(precision))\nprint(\"Test Accuracy of the model \\t = {:.4f}\".format(accuracy))\nprint(\"\\nTest F1 score of the model \\t = {:.4f}\".format(f1_score))\nprint(\"\\nTest F2 score of the model \\t = {:.4f}\".format(f2_score))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:29.13028Z","iopub.execute_input":"2021-11-22T12:26:29.130607Z","iopub.status.idle":"2021-11-22T12:26:29.147993Z","shell.execute_reply.started":"2021-11-22T12:26:29.13054Z","shell.execute_reply":"2021-11-22T12:26:29.146912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Result\"></a>\n## 5. Results\n> + 由以上結果可知：經過 30 epochs (i.e., Stage 1 + Stage 2) 的訓練，**Recall** 值接近 100%；這代表預測模型的 false-negative 誤判部分 (亦即，將 Pneumonia 誤判成 Normal 的狀況) 將會下降趨近 0。\n> + 然而， **F1 score** 仍然還有改進的空間；這是因為 false-positive 誤診部分 (亦即，將 Normal 誤判成 Pneumonia 的狀況) 造成 **Precision** 的精確值偏低的緣故。\n> + 請想想看，如何改進模型或調校預測模型參數 (hyperparameters)，進而能夠使其 **F1 score** 數值上升。  Good luck！\n\n以下將 From-Coarse-to-Fine 全程訓練過程的 Learning Curves 繪製如下，做為參考：","metadata":{}},{"cell_type":"code","source":"acc_total = history_no_data_aug.history['accuracy'] + history_data_aug.history['accuracy']\nval_acc_total = history_no_data_aug.history['val_accuracy'] + history_data_aug.history['val_accuracy']\n\nloss_total = history_no_data_aug.history['loss'] + history_data_aug.history['loss']\nval_loss_total = history_no_data_aug.history['val_loss'] + history_data_aug.history['val_loss']","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:29.153335Z","iopub.execute_input":"2021-11-22T12:26:29.153774Z","iopub.status.idle":"2021-11-22T12:26:29.164271Z","shell.execute_reply.started":"2021-11-22T12:26:29.153602Z","shell.execute_reply":"2021-11-22T12:26:29.163516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_epochs = epochs_stage_1\ntotal_epochs = epochs_stage_1 + epochs_stage_2\nepochs_range = range(1, total_epochs + 1)\n\nplt.figure(figsize=(10, 10))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc_total, label='Training Accuracy')\nplt.plot(epochs_range, val_acc_total, label='Validation Accuracy')\nplt.ylim([0, 1])\nplt.xticks(range(1,total_epochs+1,1))\nplt.plot([initial_epochs,initial_epochs],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss_total, label='Training Loss')\nplt.plot(epochs_range, val_loss_total, label='Validation Loss')\nplt.ylim([0, 1])\nplt.xticks(range(1,total_epochs+1,1))\nplt.plot([initial_epochs,initial_epochs],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:29.169661Z","iopub.execute_input":"2021-11-22T12:26:29.170083Z","iopub.status.idle":"2021-11-22T12:26:29.790993Z","shell.execute_reply.started":"2021-11-22T12:26:29.16992Z","shell.execute_reply":"2021-11-22T12:26:29.790185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"SavingEntireModel\"></a>\n## 6. Saving the Entire Model with HDF5 Format","metadata":{}},{"cell_type":"code","source":"# Saving the entire model to a HDF5 file：\n# The '.h5' extension is for the HDF5 format.\nmodel.save('PD_HDF5_model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:29.792273Z","iopub.execute_input":"2021-11-22T12:26:29.792738Z","iopub.status.idle":"2021-11-22T12:26:30.079507Z","shell.execute_reply.started":"2021-11-22T12:26:29.792687Z","shell.execute_reply":"2021-11-22T12:26:30.078684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reloading the HDF5 model, including its weights and the optimizer.\nHDF5_model = tf.keras.models.load_model('PD_HDF5_model.h5')\n\n# Show the model architecture\nHDF5_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:30.080935Z","iopub.execute_input":"2021-11-22T12:26:30.081207Z","iopub.status.idle":"2021-11-22T12:26:33.999769Z","shell.execute_reply.started":"2021-11-22T12:26:30.081161Z","shell.execute_reply":"2021-11-22T12:26:33.999032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the restored HDF5 model\nloss, acc = HDF5_model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', loss)\nprint('Test accuracy:', acc)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:34.003703Z","iopub.execute_input":"2021-11-22T12:26:34.003935Z","iopub.status.idle":"2021-11-22T12:26:35.481109Z","shell.execute_reply.started":"2021-11-22T12:26:34.003894Z","shell.execute_reply":"2021-11-22T12:26:35.478408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.concat([pd.Series(range(1,(len(pred)+1)),name = \"ImageId\"),preds],axis = 1)\ndata_subm = {'ImageId': pd.Series(range(1,(len(y_pred)+1))), 'Prediction': y_pred}\nsubmission = pd.DataFrame(data_subm)\nsubmission = submission.applymap(str)\n\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:26:35.48554Z","iopub.execute_input":"2021-11-22T12:26:35.487809Z","iopub.status.idle":"2021-11-22T12:26:35.717628Z","shell.execute_reply.started":"2021-11-22T12:26:35.487756Z","shell.execute_reply":"2021-11-22T12:26:35.716795Z"},"trusted":true},"execution_count":null,"outputs":[]}]}